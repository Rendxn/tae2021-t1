---
title: "Rstudio a Colab "
output: html_notebook
---
\usepackage[hidelinks]{hyperref}

## Herramientas extra y K óptimo 

Debido a la cantidad de variables utilizadas $(10)$ ,y el tamaño final de la muestra $(93992)$ el entrenamiento y la búsqueda del $K$ optimo es una tarea complicada para desarrollarla por métodos comunes como ciclos iterativos, por lo cual, se da uso de la librería $Caret$^[1:https://topepo.github.io/caret/] de $R$, la librería, cuenta con distintos modelos para regresión y clasificación, junto con diferentes parámetros que permiten calibrar el entrenamiento al gusto del modelador ayudando a modificar el tipo de entrenamiento,  e l preprocesamiento, parámetros específicos de cada modelo (como los vecinos cercanos $K$ para KNN) ,entre otras. Para el entrenamiento de nuestro modelo por el método $KNN$ era preciso saber el valor $K$ que, aunque la librería $Caret$  lo permitía, el tiempo transcurrido en cada entrenamiento y los recursos computaciones que implicaban impedían hacer pruebas de manera continua, tardando aproximadamente $15$ minutos por entrenamiento, ya que la librería genera distintos modelos hasta encontrar el K que genere el mejor modelo entre todos, por esta razón, se optó por usar una herramienta externa que permitiera usar GPU que ayudaran a optimizar el entrenamiento de los diferentes modelos y hacer las pruebas a diferentes configuraciones. 

### Google Colab y PyCaret 

$Google Colab$ ^[1:https://colab.research.google.com/] es un entorno de desarrollo en línea de lenguaje Python que da al usuario herramientas como GPUs gratuitas, notebooks sin instalación local, acceso en línea, y disposición de librerías Python, de esta manera, $Colab$ permitió el entrenamiento de forma remota de nuestro modelo de prueba. 
Además, era necesaria una librería que trabajara de forma parecida a $Caret$, en lenguaje Python, por esto, se usa $PyCaret$ ^[2:https://pycaret.org/] , esta es una librería de acceso libre enfocada en machine learning con el propósito de implementar las herramientas de $Caret$ en un entorno Python, funcionando de la misma forma y manteniendo su simplicidad en aplicación.
Dando uso de estas herramientas se procede a buscar el K optimo subiendo la base de datos a un notebook de $Colab$ como un dataframe, y por medio de $Pycaret$  se procede a entrenar el modelo con los siguientes parámetros:

* categorical_features : Se señala las columnas que son categóricas
* numerical_features: Se señalan las columnas que son numéricas 
* Target: Se establece la columna objetivo o de etiqueta para el modelo 
* use_gpu: Se habilita el uso de GPU para el entrenamiento 

De esta forma  la librería, reconoce el tipo de dato ingresado como categórico o numérico,  genera  conjunto de entrenamiento  de $47981$ datos, y un conjunto de test de $20564$, haciendo una distribución $70\%-30\%$ respectivamente, los conjuntos formados llevan un preprocesamiento donde normaliza las variables numéricas y permite el correcto funcionamiento del modelo, generando un $K$ optimo igual a $5$, valor que es de gran interés puesto que es el valor que se asignara a la librería $Caret$ y generará el modelo final.


